{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Remove stopwords </h2>  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import spacy\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('./data/rspct.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed = set() # Removed stopwords and alpha chars. Save in --> nostopwords_df\n",
    "pos_processed = set() # PoS tag + remove stopwords\n",
    " \n",
    "def pos_preprocess(text):\n",
    "    # Returns PoS-tag for text\n",
    "    doc = nlp(text)\n",
    "    return [token.pos_ for token in doc]\n",
    "\n",
    "def text_preprocess(text):\n",
    "    # Returns PoS-tag for text\n",
    "    doc = nlp(text)\n",
    "    txt = [token.text for token in doc if (not token.is_stop or not token.is_alpha)]\n",
    "    return \" \".join(txt)\n",
    "\n",
    "def preprocess(data_df):\n",
    "    count = 0\n",
    "    threshold = 10000\n",
    "    print(\"Starting - Log every 10k parsed\")\n",
    "\n",
    "    for row in data_df.itertuples():\n",
    "        title_processed = text_preprocess(row.title)\n",
    "        selftext_processed = text_preprocess(row.selftext)\n",
    "        feature_title = list(map(lambda x: \"title_\" + x, title_processed.split()))\n",
    "        feature_text = list(map(lambda x: \"body_\" + x, selftext_processed.split()))\n",
    "        #duplicates = [word for word in feature_title if \"body_\" + word.split(\"_\")[0] in feature_text]\n",
    "        \n",
    "        #new_col = feature_title + feature_text + duplicates\n",
    "        new_col = feature_title + feature_text\n",
    "\n",
    "        new_col_stringify = \" \".join(new_col)\n",
    "        pre_processed.add((row.id, row.subreddit, new_col_stringify))\n",
    "        \n",
    "        if count >= threshold:\n",
    "            print(count)\n",
    "            threshold += 10000\n",
    "        count+=1\n",
    "def pos_parse(data_df):\n",
    "    # Takes to long...\n",
    "    count = 0\n",
    "    threshold = 10000\n",
    "    print(\"Starting - Log every 10k parsed\")\n",
    "    for row in data_df.itertuples():\n",
    "        title_processed = text_preprocess(row.title)\n",
    "        selftext_processed = text_preprocess(row.selftext)\n",
    "        pos_title = pos_preprocess(title_processed)\n",
    "        pos_selftext = pos_preprocess(selftext_processed)        \n",
    "        #new_col = feature_title + feature_text + duplicates\n",
    "        #new_col_stringify = \" \".join(new_col)\n",
    "        if count >= threshold:\n",
    "            print(count)\n",
    "            threshold += 10000\n",
    "        pos_processed.add((row.id, row.subreddit, title_processed, \" \".join(pos_title), selftext_processed, \" \".join(pos_selftext)))\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Create DF with removed stopwords and alpha and save to csv.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess(data_df)\n",
    "processed_stopwords_df = pd.DataFrame(pre_processed, columns=(\"id\", \"subreddit\", \"body\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_opts = dict(method='zip',\n",
    "                        archive_name='preprocess_stopwords.csv')  \n",
    "processed_stopwords_df.to_csv('preprocess_stopwords.zip', index=False,\n",
    "          compression=compression_opts)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Extension on previous - Additionally add PoS-tag</h3>\n",
    "Takes hours to run..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting - Log every 10k parsed\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "400000\n",
      "410000\n",
      "420000\n",
      "430000\n",
      "440000\n",
      "450000\n",
      "460000\n",
      "470000\n",
      "480000\n",
      "490000\n",
      "500000\n",
      "510000\n",
      "520000\n",
      "530000\n",
      "540000\n",
      "550000\n",
      "560000\n",
      "570000\n",
      "580000\n",
      "590000\n",
      "600000\n",
      "610000\n",
      "620000\n",
      "630000\n",
      "640000\n",
      "650000\n",
      "660000\n",
      "670000\n",
      "680000\n",
      "690000\n",
      "700000\n",
      "710000\n",
      "720000\n",
      "730000\n",
      "740000\n",
      "750000\n",
      "760000\n",
      "770000\n",
      "780000\n",
      "790000\n",
      "800000\n",
      "810000\n",
      "820000\n",
      "830000\n",
      "840000\n",
      "850000\n",
      "860000\n",
      "870000\n",
      "880000\n",
      "890000\n",
      "900000\n",
      "910000\n",
      "920000\n",
      "930000\n",
      "940000\n",
      "950000\n",
      "960000\n",
      "970000\n",
      "980000\n",
      "990000\n",
      "1000000\n",
      "1010000\n"
     ]
    }
   ],
   "source": [
    "pos_parse(data_df)\n",
    "processed_pos_df = pd.DataFrame(pos_processed, columns=(\"id\", \"subreddit\", \"title\", \"title_pos\", \"selftext\", \"selftext_pos\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_opts = dict(method='zip',\n",
    "                        archive_name='preprocessed_stop_pos.csv')  \n",
    "processed_pos_df.to_csv('preprocessed_stop_pos.zip', index=False,\n",
    "          compression=compression_opts)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RedditClassification",
   "language": "python",
   "name": "redditclassification"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
