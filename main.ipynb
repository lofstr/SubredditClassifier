{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import spacy\n",
    "import time\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from textblob import TextBlob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nostopwords_df = pd.read_csv('./data/preprocessed_stop_pos.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change rows with no info to UNK or NAN for text and pos respectively.\n",
    "new_df = set()\n",
    "for row in nostopwords_df.itertuples():\n",
    "    title = row.title\n",
    "    title_pos = row.title_pos\n",
    "    selftext = row.selftext\n",
    "    selftext_pos = row.selftext_pos\n",
    "    if type(row.title) == float:\n",
    "        title = \"UNK\"\n",
    "        title_pos = \"NAN\"\n",
    "    if type(row.selftext) == float:\n",
    "        selftext = \"UNK\"\n",
    "        selftext_pos = \"NAN\"\n",
    "    \n",
    "    new_df.add((row.subreddit, title,title_pos, selftext,selftext_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrubbed = pd.DataFrame(new_df, columns=[\"subreddit\", \"title\", \"title_pos\", \"selftext\", \"selftext_pos\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Featurizer </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuf_set = set()\n",
    "\n",
    "def cuf(df):\n",
    "    c = []\n",
    "    count = 0\n",
    "    for row in df.itertuples():\n",
    "        # ------------------------- DATA ---------------------------\n",
    "        title = row.title.split()\n",
    "        body = row.selftext.split()\n",
    "        t1=row.title_pos.split()\n",
    "        t2=row.selftext_pos.split()\n",
    "        \n",
    "        s1 = list(map(lambda x: \"title_\" + x, row.title.split()))\n",
    "        s2 = list(map(lambda x: \"body_\" + x, row.selftext.split()))\n",
    "        feature_pos_title = list(map(lambda x: \"title_pos_\" + x, row.title_pos.split()))\n",
    "        feature_pos_text = list(map(lambda x: \"body_pos_\" + x, row.selftext_pos.split()))\n",
    "        \n",
    "        # ---------------------- Baseline ---------------------------------\n",
    "        baseline = s1 + s2\n",
    "        baseline_string = \" \".join(baseline)\n",
    "        \n",
    "        # ---------------------- FIRST POS == PUNCT ----------------------\n",
    "        punct_ft = \"\"\n",
    "        if feature_pos_title[0] == \"title_pos_PUNCT\":\n",
    "            punct_ft += \"firstcharispunct\"\n",
    "        \n",
    "        # ---------------------- POS-TRIGRAMS & avg. word len -------------------\n",
    "        pos_trigram_title = \"\"\n",
    "        pos_trigram_body = \"\"\n",
    "\n",
    "        for i in range(len(feature_pos_title[:-2])):\n",
    "            pos_trigram = \" \" + \"_\".join(feature_pos_title[i:i+3])\n",
    "            pos_trigram_title += pos_trigram\n",
    "        \n",
    "        #truncated_body = feature_pos_text[:50]\n",
    "        #for i in range(len(truncated_body[:-2])):\n",
    "        #    pos_trigram = \" \" + \"_\".join(feature_pos_text[i:i+3])\n",
    "        #    pos_trigram_body += pos_trigram\n",
    "        \n",
    "        # ------------------- TextBlob Sentiment and Polarity ----------------------\n",
    "        blob = False # Takes a long time, can't do every run.\n",
    "        if blob == True:\n",
    "            sent = TextBlob(\" \".join(body))\n",
    "            subjectivity_ft = \"\"\n",
    "            polarity_ft = \"\"\n",
    "            polarity = sent.sentiment.polarity\n",
    "            subjectivity = sent.sentiment.subjectivity\n",
    "\n",
    "            if polarity >= 0.65: polarity_ft += \" polaritysixtyfivepercent\"\n",
    "            elif polarity <= -0.60: polarity_ft += \" polarityreallylow\"\n",
    "\n",
    "            if subjectivity >= 0.80: subjectivity_ft += \" subjectivityseventyfivepercent\"\n",
    "            elif subjectivity <= 0.20: subjectivity_ft += \" subjectivitylesthantwentyfivepercent\"\n",
    "\n",
    "        \n",
    "        # ----------------------- Avg word length -------------------\n",
    "        average_word_length_title = sum(len(word)-6 for word in s1) / len(s1)\n",
    "        average_word_length_body = sum(len(word)-5 for word in s2) / len(s2)\n",
    "        avg_len_title_string = \"\"\n",
    "        avg_len_body_string = \"\"\n",
    "        avg_len_title_ft = \"\"\n",
    "        avg_len_body_ft = \"\"\n",
    "        \n",
    "        if average_word_length_title >= 7: avg_len_title_string += \" avg_wordlentitleseven\"\n",
    "        elif average_word_length_title >= 6: avg_len_title_string += \" avg_wordlentitlesix\"\n",
    "        elif average_word_length_title >= 5: avg_len_title_string += \" avg_wordlentitlefive\"\n",
    "        elif average_word_length_title >= 4: avg_len_title_string += \" avg_wordlentitlefour\"\n",
    "        elif average_word_length_title >= 3: avg_len_title_string += \" avg_wordlentitlethree\"\n",
    "        elif average_word_length_title >= 2: avg_len_title_string += \" avg_wordlentitletwo\"\n",
    "            \n",
    "        for i in range(1):\n",
    "            avg_len_title_ft += avg_len_title_string\n",
    "\n",
    "        if average_word_length_body >= 7: avg_len_body_string += \" avg_wordlenbodyeseven\"\n",
    "        elif average_word_length_body >= 6: avg_len_body_string += \" avg_wordlenbodyesix\"\n",
    "        elif average_word_length_body >= 5: avg_len_body_string += \" avg_wordlenbodyfive\"\n",
    "        elif average_word_length_body >= 4: avg_len_body_string += \" avg_wordlenbodyfour\"\n",
    "        elif average_word_length_body >= 3: avg_len_body_string += \" avg_wordlenbodythree\"\n",
    "        elif average_word_length_body >= 2: avg_len_body_string += \" avg_wordlenbodytwo\"\n",
    "            \n",
    "        for i in range(1):\n",
    "            avg_len_body_ft += avg_len_body_string\n",
    "\n",
    "        # ----------------- Symbol feature ---------------------\n",
    "        symbol_ft = \"\"\n",
    "        if feature_pos_title.count(\"title_pos_SYM\") >= 4:\n",
    "            for i in range(1):\n",
    "                symbol_ft += \" SYMBOL_EXIST\"\n",
    "                \n",
    "        if feature_pos_text.count(\"body_pos_SPACE\") >= 4:\n",
    "            for i in range(1):\n",
    "                symbol_ft += \" SPACE_EXIST\"\n",
    "                \n",
    "        # ----------------- Nominalkvot -----------------\n",
    "        noun_verb_ratio = \"\"\n",
    "        nouns = 0\n",
    "        verbs = 0\n",
    "        for pos in row.selftext_pos.split():\n",
    "            if pos == \"VERB\" or pos == \"ADV\" or pos == \"PRON\":\n",
    "                verbs += 1\n",
    "            elif pos == \"NOUN\" or pos == \"ADP\":\n",
    "                nouns += 1\n",
    "                \n",
    "        if verbs > 0: measure = nouns / verbs\n",
    "        else: measure = 0.5\n",
    "            \n",
    "        measure_feature = \"\"\n",
    "        if measure >= 4:   measure_feature = \" noun_verb_ratio_four\"\n",
    "        elif measure >= 3:   measure_feature = \" noun_verb_ratio_three\"\n",
    "        elif measure >= 1: measure_feature = \" noun_verb_ratio_one\"\n",
    "        elif measure < 0.5:              measure_feature = \" noun_verb_ratio_lessthanzeropointfive\"\n",
    "        for i in range(1):\n",
    "            noun_verb_ratio += measure_feature\n",
    "        \n",
    "        # ------------ TITLE AND SELFTEXT LENGTH FEATURE -----------------\n",
    "        selftext_len_ft_string = \"\"\n",
    "        selftext_len = \"\"\n",
    "        title_len_ft_string = \"\"\n",
    "        title_len = \"\"\n",
    "        if len(row.selftext.split()) >= 700: selftext_len_ft_string += \" sevenhundred_selftext_len\"\n",
    "        elif len(row.selftext.split()) >= 500: selftext_len_ft_string += \" fivehundred_selftext_len\"\n",
    "        elif len(row.selftext.split()) >= 300: selftext_len_ft_string += \" threehundred_selftext_len\"\n",
    "        elif len(row.selftext.split()) >= 100: selftext_len_ft_string += \" hundred_selftext_len\"\n",
    "        elif len(row.selftext.split()) >= 50: selftext_len_ft_string += \" fifty_selftext_len\"\n",
    "        elif len(row.selftext.split()) < 50: selftext_len_ft_string += \" less_than_five_wordsbody\"\n",
    "        for i in range(1):\n",
    "            selftext_len += selftext_len_ft_string\n",
    "\n",
    "        if len(row.title.split()) > 28: title_len_ft_string += \"title_morethantwentyeight\"\n",
    "        elif len(row.title.split()) > 15: title_len_ft_string += \"title_morethanfifteen\"\n",
    "        elif len(row.title.split()) < 5: title_len_ft_string += \"title_lessthanfive\"\n",
    "        for i in range(1):\n",
    "            title_len += title_len_ft_string\n",
    "\n",
    "        # -- Add to set ---\n",
    "        ft = \"\" # FEATURE STRING\n",
    "        ft += baseline_string\n",
    "        #print(ft)\n",
    "        #ft += avg_len_title_ft\n",
    "        #ft += avg_len_body_ft\n",
    "        #ft += title_bigram\n",
    "        #ft += pos_trigram_title\n",
    "        #ft += pos_trigram_body\n",
    "        #ft += symbol_ft\n",
    "        #ft += noun_verb_ratio\n",
    "        #ft += selftext_len\n",
    "        #ft += title_len\n",
    "        #ft += subjectivity_ft\n",
    "        #ft += polarity_ft\n",
    "        #ft += punct_ft\n",
    "        cuf_set.add((row.subreddit, ft))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuf(scrubbed)\n",
    "cuf_df = pd.DataFrame(cuf_set, columns=[\"subreddit\", \"body\"])\n",
    "cuf_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cut, test_cut = train_test_split(cuf_df, test_size=0.2, shuffle=True, random_state = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "682.8519630432129 seconds\n"
     ]
    }
   ],
   "source": [
    "# Cross Unigram (CU)\n",
    "start = time.time()\n",
    "#--------------------\n",
    "cu_pipe = Pipeline([\n",
    "    (\"tf\", TfidfVectorizer(min_df=1, max_features = None, ngram_range=(1,1))),\n",
    "    (\"sel\", SelectKBest(chi2, k=100000)),\n",
    "    #('vect', HashingVectorizer(binary=True)),\n",
    "    ('mnb', MultinomialNB()),\n",
    "])\n",
    "cu_pipe.fit(train_cut.body, train_cut.subreddit)\n",
    "#--------------------\n",
    "end = time.time()\n",
    "print(end - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.382012128829956 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "#--------------------\n",
    "pred = cu_pipe.predict(test_cut.body)\n",
    "#--------------------\n",
    "end = time.time()\n",
    "print(end - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7826107602749512, 0.7425370187561698, 0.7467698708575711, None)\n"
     ]
    }
   ],
   "source": [
    "score = precision_recall_fscore_support(test_cut.subreddit, pred, average='weighted')\n",
    "print(score)\n",
    "\n",
    "# -- Precision @X ---\n",
    "tf_idf_transform = cu_pipe[0].transform(test_cut.body)\n",
    "k_best = cu_pipe[1].transform(tf_idf_transform) # using chi2\n",
    "test_pred_proba = cu_pipe[2].predict_proba(k_best) # use (tf_idf_transform without chi2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(y_true, y_pred, k=5):\n",
    "    top_n_predictions = np.argsort(y_pred, axis=1)[:, -k:]\n",
    "    c = cu_pipe[2].classes_ # index 1 without chi2\n",
    "    y_true = np.array(y_true)\n",
    "    precision = 0\n",
    "    for i, prediction in enumerate(top_n_predictions):\n",
    "        for pred_index in prediction:\n",
    "            if c[pred_index] == y_true[i]:\n",
    "                precision += 1\n",
    "                break\n",
    "    \n",
    "    return precision / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision@1 = 0.7425370187561698\n",
      "precision@3 = 0.8626999012833169\n",
      "precision@5 = 0.8957453109575518\n"
     ]
    }
   ],
   "source": [
    "print('precision@1 =', precision_at_k(test_cut.subreddit, test_pred_proba, 1))\n",
    "print('precision@3 =', precision_at_k(test_cut.subreddit, test_pred_proba, 3))\n",
    "print('precision@5 =', precision_at_k(test_cut.subreddit, test_pred_proba, 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_cut.subreddit, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RedditClassification",
   "language": "python",
   "name": "redditclassification"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
